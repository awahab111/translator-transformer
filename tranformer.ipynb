{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wahab/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import os\n",
    "import re \n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Save the name of the model whose tokenizer we are using. We will need it later.\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "# Download the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pre_trained = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "embedding_layer = pre_trained.get_input_embeddings()\n",
    "embedding_layer = embedding_layer.requires_grad_(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file, tokenizer, max_len, src_lang, tgt_lang):\n",
    "        self.src_file = src_file\n",
    "        self.tgt_file = tgt_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.src_lang_text = []\n",
    "        self.tgt_lang_text = []\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self._build()\n",
    "    \n",
    "    def _build(self):\n",
    "        with open(self.src_file, 'r', encoding='utf-8') as f:\n",
    "            self.src_lang_text = f.readlines()\n",
    "        with open(self.tgt_file, 'r', encoding='utf-8') as f:\n",
    "            self.tgt_lang_text = f.readlines()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_lang_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_lang_text = self.src_lang_text[idx]\n",
    "        tgt_lang_text = self.tgt_lang_text[idx]\n",
    "        \n",
    "        tokenized_src_lang_text = self.tokenizer.encode_plus(\n",
    "            src_lang_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        tokenized_tgt_lang_text = self.tokenizer(\n",
    "            tgt_lang_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        tokenized_src_lang_text['input_ids'] = tokenized_src_lang_text['input_ids'].squeeze()\n",
    "        # src_mask = tokenized_src_lang_text['attention_mask'].squeeze()  \n",
    "        tokenized_tgt_lang_text['input_ids'] = tokenized_tgt_lang_text['input_ids'].squeeze()        \n",
    "        # tgt_mask = tokenized_tgt_lang_text['attention_mask'].squeeze()\n",
    "        \n",
    "\n",
    "        return tokenized_src_lang_text, tokenized_tgt_lang_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data files\n",
    "train_en_path = 'umc005-corpus/quran/train.en'\n",
    "train_ur_path = 'umc005-corpus/quran/train.ur'\n",
    "dev_en_path = 'umc005-corpus/quran/dev.en'\n",
    "dev_ur_path = 'umc005-corpus/quran/dev.ur'\n",
    "test_en_path = 'umc005-corpus/quran/test.en'\n",
    "test_ur_path = 'umc005-corpus/quran/test.ur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([   101,  12689,  10261,    113,  10105,    175,  12090,  25087,  81520,\n",
       "            114,  10124,  10472,  10160,  10435,  15858,  52347,  10106,    113,\n",
       "          27224,  13788,  10230,    114,  10105,  11038,  20645,    119,    113,\n",
       "          10117,  13440,  10108,  10105,  51635,  34962,  10393,  10472,  12153,\n",
       "          10142,  10957,  11178, 100745, 108787,    119,    114,    102,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([   101,  11363,  21035,  29692,    789,  11086,  32345,  14981,  35067,\n",
       "         104672,  29941,  63274,    791, 111170,  55021,  71677,    782,  38979,\n",
       "          10691,    764,  27887,  12009,  12190,  10909,  45703,  10961,    764,\n",
       "          16498,  17539,  21735,  14014,  12441,  45703,    781,  67151,  12574,\n",
       "          14269,  10691,    787,  26649,  40634,  81415,  21735,    818,  25908,\n",
       "          11145,  35075,    837,    102,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 256\n",
    "train = TranslationDataset(dev_en_path, dev_ur_path, tokenizer, max_len, 'en', 'ur')\n",
    "# train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_word(input_ids, tokenizer):\n",
    "    return tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for English text:\n",
      "['[CLS]', 'So', ',', 'continuously', 'ad', '##moni', '##sh', 'them', ',', 'for', 'you', 'are', 'but', 'an', 'ad', '##moni', '##sher', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Tokens for Urdu text:\n",
      "['[CLS]', 'پس', 'آپ', 'ن', '##ص', '##ی', '##حت', 'ف', '##رم', '##ات', '##ے', 'ر', '##ہ', '##ئے', '،', 'آپ', 'تو', 'ن', '##ص', '##ی', '##حت', 'ہی', 'ف', '##رمان', '##ے', 'والے', 'ہیں', '۔', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For English text\n",
    "for batch in train_loader:\n",
    "    tokens_en, tokens_ur = batch\n",
    "    input_ids_en = tokens_en['input_ids'][0]\n",
    "    tokens_en_tokens = to_word(input_ids_en, tokenizer)\n",
    "\n",
    "    print(\"Tokens for English text:\")\n",
    "    print(tokens_en_tokens)\n",
    "\n",
    "    # For Urdu text\n",
    "    input_ids_ur = tokens_ur['input_ids'][0]\n",
    "    tokens_ur_tokens = tokenizer.convert_ids_to_tokens(input_ids_ur)\n",
    "\n",
    "    print(\"Tokens for Urdu text:\")\n",
    "    print(tokens_ur_tokens)\n",
    "    print()\n",
    "    break  # Remove this break if you want to process the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(512, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings = pre_trained.bert.embeddings.position_embeddings\n",
    "pos_embeddings = pos_embeddings.requires_grad_(False)\n",
    "pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, pos_embeddings):\n",
    "        super(LearnedPositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = pos_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_embed = self.pos_embedding(positions)\n",
    "        return x + pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, pad_token_id=None, apply_mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask = apply_mask\n",
    "\n",
    "        # Linear layers to project queries, keys, and values\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # Output linear layer\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, input_ids=None):\n",
    "        batch_size, seq_length, embed_size = q.size()\n",
    "        \n",
    "        queries = self.query(q)\n",
    "        keys = self.key(k)\n",
    "        values = self.value(v)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply causal mask for decoder self-attention\n",
    "        if self.mask:\n",
    "            causal_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).to(device)  # Shape: [1, 1, seq_length, seq_length]\n",
    "            attention_scores = attention_scores.masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "        # Apply padding mask to ignore padding tokens\n",
    "        if input_ids is not None and self.pad_token_id is not None:\n",
    "            padding_mask = (input_ids == self.pad_token_id).unsqueeze(1).unsqueeze(2)  # Shape: [batch_size, 1, 1, seq_length]\n",
    "            attention_scores = attention_scores.masked_fill(padding_mask, float('-inf'))\n",
    "\n",
    "        # Calculate attention weights and apply to values\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        out = torch.matmul(attention_weights, values)  # Shape: [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        # Reshape back to [batch_size, seq_length, embed_size]\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_size, num_heads, pad_token_id=None, apply_mask=False):\n",
    "#         super(MultiHeadAttention, self).__init__()\n",
    "#         assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
    "\n",
    "#         self.embed_size = embed_size\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = embed_size // num_heads\n",
    "#         self.pad_token_id = pad_token_id\n",
    "#         self.mask = apply_mask\n",
    "\n",
    "#         # Linear layers to project queries, keys, and values\n",
    "#         self.query = nn.Linear(embed_size, embed_size)\n",
    "#         self.key = nn.Linear(embed_size, embed_size)\n",
    "#         self.value = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "#         # Output linear layer\n",
    "#         self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "#     def forward(self, q, k, v, input_ids=None):\n",
    "#         batch_size, seq_length, embed_size = q.size()\n",
    "        \n",
    "#         queries = self.query(q)\n",
    "#         keys = self.key(k)\n",
    "#         values = self.value(v)\n",
    "\n",
    "#         # Reshape for multi-head attention\n",
    "#         queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#         keys = keys.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#         values = values.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "#         # Scaled Dot-Product Attention\n",
    "#         attention_scores = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** 0.5)\n",
    "\n",
    "#         # Apply causal mask for decoder self-attention\n",
    "#         if self.mask:\n",
    "#             causal_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "#             causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).to(x.device)  # Shape: [1, 1, seq_length, seq_length]\n",
    "#             attention_scores = attention_scores.masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "#         # Apply padding mask to ignore padding tokens\n",
    "#         if input_ids is not None and self.pad_token_id is not None:\n",
    "#             padding_mask = (input_ids == self.pad_token_id).unsqueeze(1).unsqueeze(2)  # Shape: [batch_size, 1, 1, seq_length]\n",
    "#             attention_scores = attention_scores.masked_fill(padding_mask, float('-inf'))\n",
    "\n",
    "#         # Calculate attention weights and apply to values\n",
    "#         attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "#         out = torch.matmul(attention_weights, values)  # Shape: [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "#         # Reshape back to [batch_size, seq_length, embed_size]\n",
    "#         out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size)\n",
    "#         out = self.fc_out(out)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# # Example usage\n",
    "# embed_size = 768\n",
    "# num_heads = 8\n",
    "# batch_size = 32\n",
    "# seq_length = 128\n",
    "# pad_token_id = 0  # Assume 0 is the padding token ID\n",
    "\n",
    "# # Create dummy input tensor and encoder outputs\n",
    "# x = torch.rand(batch_size, seq_length, embed_size)\n",
    "# encoder_outputs = torch.rand(batch_size, seq_length, embed_size)\n",
    "# input_ids = torch.randint(0, 20, (batch_size, seq_length))  # Dummy input IDs\n",
    "\n",
    "# # Encoder Self-Attention\n",
    "# encoder_mha = MultiHeadAttention(embed_size, num_heads, pad_token_id=pad_token_id, apply_mask=False)\n",
    "# encoder_output = encoder_mha(x,x,x, input_ids=input_ids)\n",
    "\n",
    "# # Decoder Self-Attention\n",
    "# decoder_self_mha = MultiHeadAttention(embed_size, num_heads, pad_token_id=pad_token_id, apply_mask=True)\n",
    "# decoder_self_output = decoder_self_mha(x,x,x, input_ids=input_ids)\n",
    "\n",
    "# # Decoder Cross-Attention\n",
    "# decoder_cross_mha = MultiHeadAttention(embed_size, num_heads, pad_token_id=pad_token_id, apply_mask=False)\n",
    "# decoder_cross_output = decoder_cross_mha(x, encoder_outputs, encoder_outputs, input_ids=input_ids)\n",
    "\n",
    "# print(\"Encoder Self-Attention Output Shape:\", encoder_output.shape)\n",
    "# print(\"Decoder Self-Attention Output Shape:\", decoder_self_output.shape)\n",
    "# print(\"Decoder Cross-Attention Output Shape:\", decoder_cross_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlh = MultiHeadAttention(12, apply_mask=True)\n",
    "# q = torch.randn(32, 128, 768)\n",
    "# k = torch.randn(32, 128, 768)\n",
    "# v = torch.randn(32, 128, 768)\n",
    "\n",
    "# output = mlh.forward(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(768,num_heads,tokenizer.pad_token_id,  False)\n",
    "        self.layer_norm1 = nn.LayerNorm(768)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(768, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 768)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(768)\n",
    "\n",
    "    def forward(self, x, input_ids):\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        attention_output = self.multi_head_attention(x, x, x, input_ids)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layer_norm1(x + attention_output)\n",
    "\n",
    "        # Feed Forward\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layer_norm2(x + feed_forward_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(num_heads) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.masked_multi_head_attention = MultiHeadAttention(768,num_heads,tokenizer.pad_token_id,  True)\n",
    "        self.layer_norm1 = nn.LayerNorm(768)\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(768,num_heads,tokenizer.pad_token_id,  False)\n",
    "        self.layer_norm2 = nn.LayerNorm(768)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(768, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 768)\n",
    "        )\n",
    "        self.layer_norm3 = nn.LayerNorm(768)\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_ids, src_ids):\n",
    "\n",
    "        # Masked Multi-Head Attention\n",
    "        masked_attention_output = self.masked_multi_head_attention(x, x, x, tgt_ids)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layer_norm1(x + masked_attention_output)\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        attention_output = self.multi_head_attention(x, encoder_output, encoder_output, src_ids)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layer_norm2(x + attention_output)\n",
    "\n",
    "        # Feed Forward\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layer_norm3(x + feed_forward_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(num_heads) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, encoder_output, src_mask, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, pos_embeddings, embedding_layer):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.positional_encoding = LearnedPositionalEncoding(pos_embeddings)\n",
    "        self.embedding_layer = embedding_layer\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, num_heads)\n",
    "        self.decoder = Decoder(num_layers, num_heads)\n",
    "\n",
    "        self.linear = nn.Linear(768, tokenizer.vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    def forward(self, src_input_ids, tgt_input_ids):\n",
    "        src_embeddings = self.embedding_layer(src_input_ids)\n",
    "        src_embeddings = self.positional_encoding(src_embeddings)\n",
    "\n",
    "        encoder_output = self.encoder(src_embeddings, src_input_ids)\n",
    "\n",
    "        tgt_embeddings = self.embedding_layer(tgt_input_ids)\n",
    "        tgt_embeddings = self.positional_encoding(tgt_embeddings)\n",
    "\n",
    "        decoder_output = self.decoder(tgt_embeddings, encoder_output, tgt_input_ids, src_input_ids)\n",
    "\n",
    "        output = self.linear(decoder_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def calculate_loss(self, output, tgt_input_ids):\n",
    "        # print(output.reshape(-1, tokenizer.vocab_size).shape)\n",
    "        # print(tgt_input_ids.reshape(-1).shape)\n",
    "\n",
    "        return self.loss_fn(\n",
    "            output.reshape(-1, tokenizer.vocab_size),\n",
    "            tgt_input_ids.reshape(-1)\n",
    "        )\n",
    "    \n",
    "    def generate(self, src_input_ids, max_len=128, temperature=0.7, top_k=5):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Encode source\n",
    "            src_embeddings = self.embedding_layer(src_input_ids)\n",
    "            src_embeddings = self.positional_encoding(src_embeddings)\n",
    "            encoder_output = self.encoder(src_embeddings, src_input_ids)\n",
    "            \n",
    "            # Initialize with BOS token\n",
    "            tgt_input_ids = torch.full((src_input_ids.size(0), 1), \n",
    "                                    tokenizer.cls_token_id,\n",
    "                                    device=src_input_ids.device)\n",
    "            \n",
    "            for _ in range(max_len-1):\n",
    "                # Get target embeddings\n",
    "                tgt_embeddings = self.embedding_layer(tgt_input_ids)\n",
    "                tgt_embeddings = self.positional_encoding(tgt_embeddings)\n",
    "                \n",
    "                # Create proper masks\n",
    "                src_mask = (src_input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "                tgt_mask = self.get_tgt_mask(tgt_input_ids)\n",
    "                \n",
    "                # Decode\n",
    "                decoder_output = self.decoder(tgt_embeddings, encoder_output, src_mask, tgt_mask)\n",
    "                logits = self.linear(decoder_output[:, -1]) / temperature\n",
    "                \n",
    "                # Top-k sampling\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, k=top_k)\n",
    "                probs = F.softmax(top_k_logits, dim=-1)\n",
    "                next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = top_k_indices.gather(-1, next_token_idx)\n",
    "                \n",
    "                tgt_input_ids = torch.cat([tgt_input_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == tokenizer.sep_token_id:\n",
    "                    break\n",
    "                    \n",
    "            return tgt_input_ids\n",
    "    \n",
    "    def translate(self, src_input_ids):\n",
    "        tgt_input_ids = self.generate(src_input_ids)\n",
    "        print(tgt_input_ids)\n",
    "        tgt_text = tokenizer.decode(tgt_input_ids[0], skip_special_tokens=True)\n",
    "        return tgt_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "num_layers = 8\n",
    "num_heads = 8\n",
    "model = Transformer(num_layers, num_heads, pos_embeddings, embedding_layer).to(device)\n",
    "model = model.float()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "total_batches = len(train_loader)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    for batch in train_loader:\n",
    "        tokens_en, tokens_ur = batch\n",
    "        batch_num += 1\n",
    "        input_ids_en = tokens_en['input_ids'].to(device)\n",
    "        input_ids_ur = tokens_ur['input_ids'].to(device)\n",
    "\n",
    "        output = model(input_ids_en, input_ids_ur)\n",
    "        loss = model.calculate_loss(output, input_ids_ur)\n",
    "        # print(output)\n",
    "        # print(input_ids_ur)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} Batch:{batch_num}/{total_batches}  Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} Loss: {total_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train[0][0]['input_ids'].unsqueeze(0))\n",
    "# print(train[0][1]['input_ids']) \n",
    "print(to_word(train[0][0]['input_ids'], tokenizer))\n",
    "print(to_word(train[0][1]['input_ids'], tokenizer))\n",
    "model.translate(train[0][0]['input_ids'].unsqueeze(0).to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
